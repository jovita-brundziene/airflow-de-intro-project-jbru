{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check ipynb is running\n",
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory\n",
    "#import os\n",
    "os.chdir(\"/Users/jovita.brundziene/Python/airflow-de-intro-project-jbru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check working directory set to project root to use relative pathways later\n",
    "#pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Extract data from local to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "from arrow_pd_parser import reader\n",
    "\n",
    "#function to load and coerse parquet files to datetime format as per metadata\n",
    "def load_and_fix_parquet_with_metadata(\n",
    "    parquet_path: str,\n",
    "    metadata_path: str,\n",
    "    datetime_columns: list\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - parquet_path: Path to the Parquet file\n",
    "    - metadata_path: Path to the JSON metadata file\n",
    "    - datetime_columns: List of column names to convert to datetime\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned Pandas DataFrame\n",
    "    \"\"\"\n",
    "    #load parquet with metadata\n",
    "    df = reader.read(\n",
    "        input_path = parquet_path,\n",
    "        metadata = metadata_path,\n",
    "        parquet_expect_full_schema = False  # Allows partial schema match\n",
    "    )\n",
    "\n",
    "    # Coerce datetime columns\n",
    "    for col in datetime_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#people-part1 df\n",
    "df1 = load_and_fix_parquet_with_metadata(\n",
    "    parquet_path = \"data/example-data/people-part1.parquet\",\n",
    "    metadata_path = \"data/metadata/intro-project-metadata.json\",\n",
    "    datetime_columns = [\"Source extraction date\", \"Date of birth\"]\n",
    ")\n",
    "\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#people-part1 df\n",
    "df2 = load_and_fix_parquet_with_metadata(\n",
    "    parquet_path = \"data/example-data/people-part1.parquet\",\n",
    "    metadata_path = \"data/metadata/intro-project-metadata.json\",\n",
    "    datetime_columns = [\"Source extraction date\", \"Date of birth\"]\n",
    ")\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#people-part1 df\n",
    "df3 = load_and_fix_parquet_with_metadata(\n",
    "    parquet_path = \"data/example-data/people-part1.parquet\",\n",
    "    metadata_path = \"data/metadata/intro-project-metadata.json\",\n",
    "    datetime_columns = [\"Source extraction date\", \"Date of birth\"]\n",
    ")\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example metadata\n",
    "mojap_schema = {\n",
    "    \"name\": \"users\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"user_id\",\n",
    "            \"type\": \"integer\",\n",
    "            \"nullable\": False\n",
    "         },\n",
    "        {\n",
    "            \"name\": \"email\",\n",
    "            \"type\": \"string\",\n",
    "            \"nullable\": False\n",
    "         },\n",
    "        {\n",
    "            \"name\": \"signup_date\",\n",
    "            \"type\": \"date\",\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "mojap_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from datetime import date, datetime\n",
    "from dlt.common.libs.pydantic import pydantic_to_table_schema_columns\n",
    "\n",
    "def mojap_to_pydantic_model(schema: dict):\n",
    "    fields = schema[\"fields\"]\n",
    "    annotations = {}\n",
    "    defaults = {}\n",
    "\n",
    "    type_mapping = {\n",
    "        \"string\": str,\n",
    "        \"integer\": int,\n",
    "        \"float\": float,\n",
    "        \"boolean\": bool,\n",
    "        \"date\": date,\n",
    "        \"datetime\": datetime\n",
    "    }\n",
    "\n",
    "    for field in fields:\n",
    "        field_name = field[\"name\"]\n",
    "        field_type = type_mapping.get(field[\"type\"], str)\n",
    "        if field.get(\"nullable\", True):\n",
    "            annotations[field_name] = Optional[field_type]\n",
    "            defaults[field_name] = None\n",
    "        else:\n",
    "            annotations[field_name] = field_type\n",
    "\n",
    "    model_attrs = {\"__annotations__\": annotations}\n",
    "    model_attrs.update(defaults)\n",
    "\n",
    "    return type(schema[\"name\"].capitalize() + \"Model\", (BaseModel,), model_attrs)\n",
    "\n",
    "def convert_to_dlt_schema(pydantic_model):\n",
    "    return {\n",
    "        \"name\": pydantic_model.__name__.lower(),\n",
    "        \"columns\": pydantic_to_table_schema_columns(pydantic_model)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env-intro-project)",
   "language": "python",
   "name": "env-intro-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

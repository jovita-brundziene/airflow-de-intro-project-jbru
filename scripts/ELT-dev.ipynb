{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb22512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "#check ipynb is running\n",
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad71e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory\n",
    "import os\n",
    "os.chdir(\"/Users/jovita.brundziene/Python/airflow-de-intro-project-jbru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54d05f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jovita.brundziene/Python/airflow-de-intro-project-jbru'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check working directory set to project root to use relative pathways later\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311de29b",
   "metadata": {},
   "source": [
    "To do:\n",
    "- metadata handling\n",
    "- Go through repo steps\n",
    "- Include dev/prod environment parameters\n",
    "- add parameters to config file\n",
    "- create a docker image\n",
    "- create a github action to run pipeline automatically\n",
    "- create unit tests\n",
    "- modularise code into at least config, functions and run\n",
    "- Update requirements file and build it into the script\n",
    "- Requirements lint?\n",
    "- Nice to have: package it up as a python package?\n",
    "- add logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae69106",
   "metadata": {},
   "source": [
    "### Extract data from local to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ff4ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def upload_parquet_files_to_s3(bucket_name, local_directory, s3_prefix, dry_run=True):\n",
    "    \"\"\"\n",
    "    Uploads .parquet files from a local directory to an S3 bucket under a specified prefix.\n",
    "\n",
    "    Parameters:\n",
    "        bucket_name (str): Name of the S3 bucket.\n",
    "        local_directory (str): Path to the local directory containing .parquet files.\n",
    "        s3_prefix (str): Path prefix within the S3 bucket.\n",
    "        dry_run (bool): If True, simulates the upload without actually uploading files.\n",
    "    \"\"\"\n",
    "    # Create an S3 client using boto3\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Loop through all files in the specified local directory\n",
    "    for file in os.listdir(local_directory):\n",
    "        # Only process files with a .parquet extension\n",
    "        if file.endswith('.parquet'):\n",
    "            # Construct the full local file path\n",
    "            local_path = os.path.join(local_directory, file)\n",
    "            # Define the S3 object key (i.e., path within the bucket)\n",
    "            s3_key = f'{s3_prefix}/{file}'\n",
    "\n",
    "            try:\n",
    "                # Check if the file already exists in the S3 bucket\n",
    "                s3.head_object(Bucket=bucket_name, Key=s3_key)\n",
    "                logging.info(f\"File already exists in S3: s3://{bucket_name}/{s3_key} — skipping upload.\")\n",
    "            except ClientError as e:\n",
    "                # If the error code is 404, the file does not exist — proceed with upload\n",
    "                if e.response['Error']['Code'] == '404':\n",
    "                    if dry_run:\n",
    "                        # Simulate the upload in dry run mode\n",
    "                        print(f\"[DRY RUN] Would upload: {local_path} to s3://{bucket_name}/{s3_key}\")\n",
    "                    else:\n",
    "                        # Attempt to upload the file to S3\n",
    "                        try:\n",
    "                            s3.upload_file(local_path, bucket_name, s3_key)\n",
    "                            logging.info(f\"Successfully uploaded: {local_path} to s3://{bucket_name}/{s3_key}\")\n",
    "                        except Exception as upload_error:\n",
    "                            logging.error(f\"Failed to upload: {local_path}. Error: {upload_error}\")\n",
    "                else:\n",
    "                    # Log unexpected errors during head_object check\n",
    "                    logging.error(f\"Error checking existence of {s3_key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90787f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#turn this into a config file\n",
    "upload_parquet_files_to_s3(\n",
    "    bucket_name='alpha-hmcts-de-testing-sandbox',\n",
    "    local_directory='data/example-data',\n",
    "    s3_prefix='de-intro-project-jb/path',\n",
    "    dry_run=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985f4f6",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "def list_parquet_files_from_s3(bucket_name: str, s3_prefix: str) -> list:\n",
    "    \"\"\"\n",
    "    Lists all Parquet files in a given S3 prefix using s3fs.\n",
    "\n",
    "    Parameters:\n",
    "        bucket_name (str): S3 bucket name.\n",
    "        s3_prefix (str): Prefix (folder path) in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        list: List of full S3 paths to Parquet files.\n",
    "    \"\"\"\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    s3_path = f\"s3://{bucket_name}/{s3_prefix}\"\n",
    "    files = fs.ls(s3_path)\n",
    "    return [f for f in files if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arrow_pd_parser import reader\n",
    "import pandas as pd\n",
    "\n",
    "def load_parquet_files_from_s3(bucket_name, s3_prefix):\n",
    "    \"\"\"\n",
    "    Loads and parses Parquet files from S3 using PyArrow and a custom parser.\n",
    "\n",
    "    Parameters:\n",
    "        bucket_name (str): S3 bucket name.\n",
    "        s3_prefix (str): Prefix (folder path) in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame from all Parquet files.\n",
    "    \"\"\"\n",
    "    #from your_module import list_parquet_files_from_s3  # adjust import as needed\n",
    "\n",
    "    parquet_files = list_parquet_files_from_s3(bucket_name, s3_prefix)\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in parquet_files:\n",
    "        df = reader.read(file_path)  # reader handles S3 paths directly\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c795ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Id</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Email</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Date of birth</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Source extraction date</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e09c4f4cbfEFaFd</td>\n",
       "      <td>Dawn</td>\n",
       "      <td>Trevino</td>\n",
       "      <td>clintongood@example.org</td>\n",
       "      <td>360-423-5286</td>\n",
       "      <td>1972-01-17</td>\n",
       "      <td>Teacher, primary school</td>\n",
       "      <td>2024-02-29 12:30:10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D781D28b845Ab9D</td>\n",
       "      <td>Dale</td>\n",
       "      <td>Mcknight</td>\n",
       "      <td>clairebradshaw@example.org</td>\n",
       "      <td>9062423229</td>\n",
       "      <td>1931-01-31</td>\n",
       "      <td>Development worker, community</td>\n",
       "      <td>2024-02-29 12:30:10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eda7EcaF87b2D80</td>\n",
       "      <td>Herbert</td>\n",
       "      <td>Bean</td>\n",
       "      <td>johnnybooker@example.org</td>\n",
       "      <td>001-149-154-0679x1617</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>Ceramics designer</td>\n",
       "      <td>2024-02-29 12:30:10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E75ACea5D7AeC3e</td>\n",
       "      <td>Karen</td>\n",
       "      <td>Everett</td>\n",
       "      <td>wkhan@example.org</td>\n",
       "      <td>870.294.7563x20939</td>\n",
       "      <td>1938-06-14</td>\n",
       "      <td>Civil engineer, consulting</td>\n",
       "      <td>2024-02-29 12:30:10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9C4Df1246ddf543</td>\n",
       "      <td>Angela</td>\n",
       "      <td>Shea</td>\n",
       "      <td>reginaldgarner@example.com</td>\n",
       "      <td>242.442.2978</td>\n",
       "      <td>1971-11-22</td>\n",
       "      <td>Health and safety adviser</td>\n",
       "      <td>2024-02-29 12:30:10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           User Id First Name Last Name                       Email  \\\n",
       "0  e09c4f4cbfEFaFd       Dawn   Trevino     clintongood@example.org   \n",
       "1  D781D28b845Ab9D       Dale  Mcknight  clairebradshaw@example.org   \n",
       "2  eda7EcaF87b2D80    Herbert      Bean    johnnybooker@example.org   \n",
       "3  E75ACea5D7AeC3e      Karen   Everett           wkhan@example.org   \n",
       "4  9C4Df1246ddf543     Angela      Shea  reginaldgarner@example.com   \n",
       "\n",
       "                   Phone Date of birth                      Job Title  \\\n",
       "0           360-423-5286    1972-01-17        Teacher, primary school   \n",
       "1             9062423229    1931-01-31  Development worker, community   \n",
       "2  001-149-154-0679x1617    2018-02-10              Ceramics designer   \n",
       "3     870.294.7563x20939    1938-06-14     Civil engineer, consulting   \n",
       "4           242.442.2978    1971-11-22      Health and safety adviser   \n",
       "\n",
       "  Source extraction date  Index  \n",
       "0    2024-02-29 12:30:10      1  \n",
       "1    2024-02-29 12:30:10      2  \n",
       "2    2024-02-29 12:30:10      3  \n",
       "3    2024-02-29 12:30:10      4  \n",
       "4    2024-02-29 12:30:10      5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "bucket = \"alpha-hmcts-de-testing-sandbox\"\n",
    "prefix = \"de-intro-project-jb/path\"\n",
    "\n",
    "df = load_parquet_files_from_s3(bucket, prefix)\n",
    "df.head()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env-intro-project)",
   "language": "python",
   "name": "env-intro-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

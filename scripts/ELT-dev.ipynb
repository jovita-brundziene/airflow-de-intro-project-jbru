{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dcb22512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "#check ipynb is running\n",
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aad71e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory\n",
    "import os\n",
    "os.chdir(\"/Users/jovita.brundziene/Python/airflow-de-intro-project-jbru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "54d05f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jovita.brundziene/Python/airflow-de-intro-project-jbru'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check working directory set to project root to use relative pathways later\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311de29b",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Go through repo steps\n",
    "- Include dev/prod environment parameters\n",
    "- add parameters to config file\n",
    "- create a docker image\n",
    "- create a github action to run pipeline automatically\n",
    "- create unit tests\n",
    "- modularise code into at least config, functions and run\n",
    "- Update requirements file and build it into the script\n",
    "- Requirements lint?\n",
    "- Nice to have: package it up as a python package?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae69106",
   "metadata": {},
   "source": [
    "### Extract data from local to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3ff4ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def upload_parquet_files_to_s3(bucket_name, local_directory, s3_prefix, dry_run=True):\n",
    "    \"\"\"\n",
    "    Uploads .parquet files from a local directory to an S3 bucket under a specified prefix.\n",
    "\n",
    "    Parameters:\n",
    "        bucket_name (str): Name of the S3 bucket.\n",
    "        local_directory (str): Path to the local directory containing .parquet files.\n",
    "        s3_prefix (str): Path prefix within the S3 bucket.\n",
    "        dry_run (bool): If True, simulates the upload without actually uploading files.\n",
    "    \"\"\"\n",
    "    # Create an S3 client using boto3\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Loop through all files in the specified local directory\n",
    "    for file in os.listdir(local_directory):\n",
    "        # Only process files with a .parquet extension\n",
    "        if file.endswith('.parquet'):\n",
    "            # Construct the full local file path\n",
    "            local_path = os.path.join(local_directory, file)\n",
    "            # Define the S3 object key (i.e., path within the bucket)\n",
    "            s3_key = f'{s3_prefix}/{file}'\n",
    "\n",
    "            try:\n",
    "                # Check if the file already exists in the S3 bucket\n",
    "                s3.head_object(Bucket=bucket_name, Key=s3_key)\n",
    "                logging.info(f\"File already exists in S3: s3://{bucket_name}/{s3_key} — skipping upload.\")\n",
    "            except ClientError as e:\n",
    "                # If the error code is 404, the file does not exist — proceed with upload\n",
    "                if e.response['Error']['Code'] == '404':\n",
    "                    if dry_run:\n",
    "                        # Simulate the upload in dry run mode\n",
    "                        print(f\"[DRY RUN] Would upload: {local_path} to s3://{bucket_name}/{s3_key}\")\n",
    "                    else:\n",
    "                        # Attempt to upload the file to S3\n",
    "                        try:\n",
    "                            s3.upload_file(local_path, bucket_name, s3_key)\n",
    "                            logging.info(f\"Successfully uploaded: {local_path} to s3://{bucket_name}/{s3_key}\")\n",
    "                        except Exception as upload_error:\n",
    "                            logging.error(f\"Failed to upload: {local_path}. Error: {upload_error}\")\n",
    "                else:\n",
    "                    # Log unexpected errors during head_object check\n",
    "                    logging.error(f\"Error checking existence of {s3_key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90787f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn this into a config file\n",
    "upload_parquet_files_to_s3(\n",
    "    bucket_name='alpha-hmcts-de-testing-sandbox',\n",
    "    local_directory='data/example-data',\n",
    "    s3_prefix='de-intro-project-jb/dev',\n",
    "    dry_run=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985f4f6",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "acbf5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "def list_parquet_files_from_s3(bucket_name: str, s3_prefix: str) -> list:\n",
    "    \"\"\"\n",
    "    Lists all Parquet files in a given S3 prefix using s3fs.\n",
    "\n",
    "    Parameters:\n",
    "        bucket_name (str): S3 bucket name.\n",
    "        s3_prefix (str): Prefix (folder path) in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        list: List of full S3 paths to Parquet files.\n",
    "    \"\"\"\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    s3_path = f\"s3://{bucket_name}/{s3_prefix}\"\n",
    "    files = fs.ls(s3_path)\n",
    "    return [f for f in files if f.endswith('.parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "29cfcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arrow_pd_parser import reader\n",
    "import pandas as pd\n",
    "\n",
    "def load_parquet_files_from_s3(bucket_name, s3_prefix):\n",
    "    \"\"\"\n",
    "    Loads and parses Parquet files from S3 using PyArrow and a custom parser.\n",
    "\n",
    "    Parameters:\n",
    "        bucket_name (str): S3 bucket name.\n",
    "        s3_prefix (str): Prefix (folder path) in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame from all Parquet files.\n",
    "    \"\"\"\n",
    "    #from your_module import list_parquet_files_from_s3  # adjust import as needed\n",
    "\n",
    "    parquet_files = list_parquet_files_from_s3(bucket_name, s3_prefix)\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in parquet_files:\n",
    "        df = reader.read(file_path)  # reader handles S3 paths directly\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c795ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "bucket = \"alpha-hmcts-de-testing-sandbox\"\n",
    "prefix = \"de-intro-project-jb/dev\"\n",
    "\n",
    "df = load_parquet_files_from_s3(bucket, prefix)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1ccd3",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "33bea63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "from arrow_pd_parser import reader\n",
    "\n",
    "#function to load parquet files\n",
    "def load_parquet(\n",
    "    parquet_path: str\n",
    "\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - parquet_path: Path to the Parquet file\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned Pandas DataFrame\n",
    "    \"\"\"\n",
    "    #load parquet with metadata\n",
    "    df = reader.read(\n",
    "        input_path = parquet_path\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d0f7b1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Id</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Email</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Date of birth</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Source extraction date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e09c4f4cbfEFaFd</td>\n",
       "      <td>Dawn</td>\n",
       "      <td>Trevino</td>\n",
       "      <td>clintongood@example.org</td>\n",
       "      <td>360-423-5286</td>\n",
       "      <td>1972-01-17</td>\n",
       "      <td>Teacher, primary school</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D781D28b845Ab9D</td>\n",
       "      <td>Dale</td>\n",
       "      <td>Mcknight</td>\n",
       "      <td>clairebradshaw@example.org</td>\n",
       "      <td>9062423229</td>\n",
       "      <td>1931-01-31</td>\n",
       "      <td>Development worker, community</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eda7EcaF87b2D80</td>\n",
       "      <td>Herbert</td>\n",
       "      <td>Bean</td>\n",
       "      <td>johnnybooker@example.org</td>\n",
       "      <td>001-149-154-0679x1617</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>Ceramics designer</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E75ACea5D7AeC3e</td>\n",
       "      <td>Karen</td>\n",
       "      <td>Everett</td>\n",
       "      <td>wkhan@example.org</td>\n",
       "      <td>870.294.7563x20939</td>\n",
       "      <td>1938-06-14</td>\n",
       "      <td>Civil engineer, consulting</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9C4Df1246ddf543</td>\n",
       "      <td>Angela</td>\n",
       "      <td>Shea</td>\n",
       "      <td>reginaldgarner@example.com</td>\n",
       "      <td>242.442.2978</td>\n",
       "      <td>1971-11-22</td>\n",
       "      <td>Health and safety adviser</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               User Id First Name Last Name                       Email  \\\n",
       "Index                                                                     \n",
       "1      e09c4f4cbfEFaFd       Dawn   Trevino     clintongood@example.org   \n",
       "2      D781D28b845Ab9D       Dale  Mcknight  clairebradshaw@example.org   \n",
       "3      eda7EcaF87b2D80    Herbert      Bean    johnnybooker@example.org   \n",
       "4      E75ACea5D7AeC3e      Karen   Everett           wkhan@example.org   \n",
       "5      9C4Df1246ddf543     Angela      Shea  reginaldgarner@example.com   \n",
       "\n",
       "                       Phone Date of birth                      Job Title  \\\n",
       "Index                                                                       \n",
       "1               360-423-5286    1972-01-17        Teacher, primary school   \n",
       "2                 9062423229    1931-01-31  Development worker, community   \n",
       "3      001-149-154-0679x1617    2018-02-10              Ceramics designer   \n",
       "4         870.294.7563x20939    1938-06-14     Civil engineer, consulting   \n",
       "5               242.442.2978    1971-11-22      Health and safety adviser   \n",
       "\n",
       "      Source extraction date  \n",
       "Index                         \n",
       "1        2024-02-29T12:30:10  \n",
       "2        2024-02-29T12:30:10  \n",
       "3        2024-02-29T12:30:10  \n",
       "4        2024-02-29T12:30:10  \n",
       "5        2024-02-29T12:30:10  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#people-part1 df\n",
    "df1 = load_parquet(\n",
    "    parquet_path = \"data/example-data/people-part1.parquet\"\n",
    ")\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "99152a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Id                   string[python]\n",
      "First Name                string[python]\n",
      "Last Name                 string[python]\n",
      "Email                     string[python]\n",
      "Phone                     string[python]\n",
      "Date of birth             string[python]\n",
      "Job Title                 string[python]\n",
      "Source extraction date    string[python]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check data types\n",
    "print(df1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1cc985b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'https://moj-analytical-services.github.io/metadata_schema/mojap_metadata/v1.3.0.json',\n",
       " 'name': 'Intro Project',\n",
       " 'description': '',\n",
       " 'file_format': '',\n",
       " 'sensitive': False,\n",
       " 'columns': [{'name': 'User ID', 'type': 'string', 'description': ''},\n",
       "  {'name': 'First Name',\n",
       "   'type': 'string',\n",
       "   'description': 'A Users first name'},\n",
       "  {'name': 'Last Name', 'type': 'string', 'description': 'A Users last name'},\n",
       "  {'name': 'Email', 'type': 'string', 'description': 'A Users email address'},\n",
       "  {'name': 'Phone', 'type': 'string', 'description': 'A Users phone number'},\n",
       "  {'name': 'Date of birth',\n",
       "   'type': 'timestamp(s)',\n",
       "   'datetime_format': '%Y-%m-%dT%H:%M:%S',\n",
       "   'description': 'A Users date of birth'},\n",
       "  {'name': 'Job Title', 'type': 'string', 'description': 'A Users job title'},\n",
       "  {'name': 'Source extraction date',\n",
       "   'type': 'timestamp(s)',\n",
       "   'datetime_format': '%Y-%m-%dT%H:%M:%S',\n",
       "   'description': \"Timestamp for start of record's presence in database\"}],\n",
       " 'primary_key': ['User ID'],\n",
       " 'partitions': []}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load metadata function\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_metadata(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load metadata from a JSON file located in the data/metadata folder.\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(\"data\", \"metadata\", filename)\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Example usage:\n",
    "metadata = load_metadata(\"intro-project-metadata.json\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f0427f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intro Project\n"
     ]
    }
   ],
   "source": [
    "#create a metadata class and validate\n",
    "from mojap_metadata import Metadata\n",
    "\n",
    "# Create Metadata object from JSON\n",
    "metadata_obj = Metadata.from_dict(metadata)\n",
    "\n",
    "# Check the object\n",
    "print(metadata_obj.name)          # Project name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "075ac1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate metadata against schema\n",
    "metadata_obj.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3e14012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>job_title</th>\n",
       "      <th>source_extraction_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e09c4f4cbfEFaFd</td>\n",
       "      <td>Dawn</td>\n",
       "      <td>Trevino</td>\n",
       "      <td>clintongood@example.org</td>\n",
       "      <td>360-423-5286</td>\n",
       "      <td>1972-01-17</td>\n",
       "      <td>Teacher, primary school</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D781D28b845Ab9D</td>\n",
       "      <td>Dale</td>\n",
       "      <td>Mcknight</td>\n",
       "      <td>clairebradshaw@example.org</td>\n",
       "      <td>9062423229</td>\n",
       "      <td>1931-01-31</td>\n",
       "      <td>Development worker, community</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eda7EcaF87b2D80</td>\n",
       "      <td>Herbert</td>\n",
       "      <td>Bean</td>\n",
       "      <td>johnnybooker@example.org</td>\n",
       "      <td>001-149-154-0679x1617</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>Ceramics designer</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E75ACea5D7AeC3e</td>\n",
       "      <td>Karen</td>\n",
       "      <td>Everett</td>\n",
       "      <td>wkhan@example.org</td>\n",
       "      <td>870.294.7563x20939</td>\n",
       "      <td>1938-06-14</td>\n",
       "      <td>Civil engineer, consulting</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9C4Df1246ddf543</td>\n",
       "      <td>Angela</td>\n",
       "      <td>Shea</td>\n",
       "      <td>reginaldgarner@example.com</td>\n",
       "      <td>242.442.2978</td>\n",
       "      <td>1971-11-22</td>\n",
       "      <td>Health and safety adviser</td>\n",
       "      <td>2024-02-29T12:30:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id first_name last_name                       email  \\\n",
       "Index                                                                     \n",
       "1      e09c4f4cbfEFaFd       Dawn   Trevino     clintongood@example.org   \n",
       "2      D781D28b845Ab9D       Dale  Mcknight  clairebradshaw@example.org   \n",
       "3      eda7EcaF87b2D80    Herbert      Bean    johnnybooker@example.org   \n",
       "4      E75ACea5D7AeC3e      Karen   Everett           wkhan@example.org   \n",
       "5      9C4Df1246ddf543     Angela      Shea  reginaldgarner@example.com   \n",
       "\n",
       "                       phone date_of_birth                      job_title  \\\n",
       "Index                                                                       \n",
       "1               360-423-5286    1972-01-17        Teacher, primary school   \n",
       "2                 9062423229    1931-01-31  Development worker, community   \n",
       "3      001-149-154-0679x1617    2018-02-10              Ceramics designer   \n",
       "4         870.294.7563x20939    1938-06-14     Civil engineer, consulting   \n",
       "5               242.442.2978    1971-11-22      Health and safety adviser   \n",
       "\n",
       "      source_extraction_date  \n",
       "Index                         \n",
       "1        2024-02-29T12:30:10  \n",
       "2        2024-02-29T12:30:10  \n",
       "3        2024-02-29T12:30:10  \n",
       "4        2024-02-29T12:30:10  \n",
       "5        2024-02-29T12:30:10  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalise column names to lowercase and replace spaces with underscores.\n",
    "def normalize_column_names(df):\n",
    "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "df1 = normalize_column_names(df1)\n",
    "\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6a11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date of birth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python/airflow-de-intro-project-jbru/env-intro-project/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Date of birth'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[198]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mError converting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df1[\u001b[33m'\u001b[39m\u001b[33mdate_of_birth\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate of birth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(convert_to_iso_timestamp)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(df1[\u001b[33m'\u001b[39m\u001b[33mdate_of_birth\u001b[39m\u001b[33m'\u001b[39m].head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python/airflow-de-intro-project-jbru/env-intro-project/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python/airflow-de-intro-project-jbru/env-intro-project/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Date of birth'"
     ]
    }
   ],
   "source": [
    "# Convert 'Date of birth' from 'YYYY-MM-DD' to ISO timestamp format 'YYYY-MM-DDTHH:MM:SS'\n",
    "def convert_to_iso_timestamp(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        # Parse date string and format as ISO timestamp\n",
    "        return pd.to_datetime(date_str, format='%Y-%m-%d').strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    except Exception as e:\n",
    "        print(f'Error converting {date_str}: {e}')\n",
    "        return None\n",
    "\n",
    "df1 = df1.apply(convert_to_iso_timestamp)\n",
    "print(df1['date_of_birth'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6e41066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enforce_metadata_types(df, metadata_obj):\n",
    "    \"\"\"\n",
    "    Enforce column data types in a DataFrame based on mojap-metadata schema.\n",
    "    Handles both dict-based and object-based columns.\n",
    "    \"\"\"\n",
    "    for col in metadata_obj.columns:\n",
    "        # Detect if col is dict or object\n",
    "        if isinstance(col, dict):\n",
    "            col_name = col[\"name\"].lower().replace(\" \", \"_\")\n",
    "            col_type = col[\"type\"]\n",
    "            fmt = col.get(\"datetime_format\", None)\n",
    "        else:  # Column-like object\n",
    "            col_name = col.name.lower().replace(\" \", \"_\")\n",
    "            col_type = col.type\n",
    "            fmt = getattr(col, \"datetime_format\", None)\n",
    "\n",
    "        if col_name not in df.columns:\n",
    "            print(f\"⚠️ Column '{col_name}' not found in DataFrame.\")\n",
    "            continue\n",
    "\n",
    "        # Apply type casting\n",
    "        if col_type == \"string\":\n",
    "            df[col_name] = df[col_name].astype(\"string\")\n",
    "        elif col_type.startswith(\"timestamp\"):\n",
    "            df[col_name] = pd.to_datetime(df[col_name], format=fmt, errors=\"coerce\")\n",
    "        elif col_type in [\"int\", \"integer\"]:\n",
    "            df[col_name] = pd.to_numeric(df[col_name], errors=\"coerce\").astype(\"Int64\")\n",
    "        elif col_type in [\"float\", \"double\"]:\n",
    "            df[col_name] = pd.to_numeric(df[col_name], errors=\"coerce\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375314f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot assemble with duplicate keys",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[189]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#enforce metadata types\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df1 = \u001b[43menforce_metadata_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[187]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36menforce_metadata_types\u001b[39m\u001b[34m(df, metadata_obj)\u001b[39m\n\u001b[32m     23\u001b[39m     df[col_name] = df[col_name].astype(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m col_type.startswith(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     df[col_name] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoerce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m col_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minteger\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     27\u001b[39m     df[col_name] = pd.to_numeric(df[col_name], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[33m\"\u001b[39m\u001b[33mInt64\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python/airflow-de-intro-project-jbru/env-intro-project/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1075\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1073\u001b[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m     result = \u001b[43m_assemble_from_unit_mappings\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Index):\n\u001b[32m   1077\u001b[39m     cache_array = _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python/airflow-de-intro-project-jbru/env-intro-project/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1170\u001b[39m, in \u001b[36m_assemble_from_unit_mappings\u001b[39m\u001b[34m(arg, errors, utc)\u001b[39m\n\u001b[32m   1168\u001b[39m arg = DataFrame(arg)\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arg.columns.is_unique:\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot assemble with duplicate keys\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1172\u001b[39m \u001b[38;5;66;03m# replace passed unit with _unit_map\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(value):\n",
      "\u001b[31mValueError\u001b[39m: cannot assemble with duplicate keys"
     ]
    }
   ],
   "source": [
    "#enforce metadata types\n",
    "df1 = enforce_metadata_types(df1, metadata_obj)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f287b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                   string[python]\n",
      "first_name                string[python]\n",
      "last_name                 string[python]\n",
      "email                     string[python]\n",
      "phone                     string[python]\n",
      "date_of_birth             datetime64[ns]\n",
      "job_title                 string[python]\n",
      "source_extraction_date    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df1.dtypes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env-intro-project)",
   "language": "python",
   "name": "env-intro-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
